{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Organization of Machine Learning Algos\n",
    "\n",
    "Ordinary Least Squares -  supervised continuous regresssion / supervised categorical classification\n",
    "\n",
    "Logistic Regression - supervised categorical classification\n",
    "\n",
    "Naive Bayes - supervised categorical classification\n",
    "\n",
    "Decision Trees - supervised continuous regression / supervised categorical classification\n",
    "\n",
    "Support Vector Machines - supervised categorical classification\n",
    "\n",
    "Nearest Neighbors - unsupervised categorical clustering\n",
    "\n",
    "K Means - unsupervised categorical clustering\n",
    "\n",
    "Principal Component Analysis (Matrix Decomposition) - unsupervised continuous dimension reduction\n",
    "\n",
    "> Examples for Algos\n",
    "\n",
    "Supervised Continuous Regression\n",
    "* Ex 1. Determine the number of bike riders based on weather data\n",
    "* Ex 2. Determine what the price of a stock index should be given various economic data\n",
    "\n",
    "Supervised Categorical Classification\n",
    "* Ex 1. Determine whether a movie is fresh based on review data\n",
    "* Ex 2. Determine whether a team will win or lose based on a current game state\n",
    "\n",
    "Unsupervised Categorical Clustering\n",
    "* Ex 1. Figure out how iris data should be grouped based on sepal and petal measurements\n",
    "* Ex 2. Determine how  people cluster based on browsing history\n",
    "\n",
    "Unsupervised Continuous Dimension Reduction\n",
    "* Ex 1. Determine how many principal components describe most of the variance in the iris data\n",
    "* Ex 2. Determine how a small group of observations differ from each other when they have a large number of features.\n",
    "\n",
    "> SKlearn Functions\n",
    "\n",
    "fit() - run algorithm on a target variable and independent set of variables\n",
    "\n",
    "transform() - selectively remove features from an algorithm based on some feature selection criteria\n",
    "\n",
    "predict() - use the results of a fitted algorithm to determine target variable values based on a matrix of observations\n",
    "\n",
    "fit_transform() - do both the transform and fit\n",
    "\n",
    "> Kernels\n",
    "\n",
    "A. A kernal is a functional transformation that can be applied to the X \n",
    "\n",
    "B. SVM and PCA\n",
    "\n",
    "> Non-linear Algo\n",
    "\n",
    "K Nearest Neighbors groups observations based on how far away an observation is from each of k number of points. It uses the mean of the squared error to move the k points.\n",
    "\n",
    "> DNA Data\n",
    "\n",
    "i. Supervised would be a Decisioon Tree with an L1 regularization. Unsupervised would be PCA.\n",
    "\n",
    "ii. The Decision Tree would determine the order of importance/and or information gained for the features and help you decide what features to pay attention to. PCA would help reduce the dimensinality of the data and give a better idea of how much of the variance can be described by some combination of features.\n",
    "\n",
    "> Gini Importance\n",
    "\n",
    "The decision tree regressor uses Gini importance. \n",
    "\n",
    "The most important feature is sqft, followed by bedrooms, then subway distance (at about the same importance) but less than half as important as sqft. Nearby pizza matters a little and bathrooms and distance columbus don't matter very much at all.\n",
    "\n",
    "> ROC AUC\n",
    "\n",
    "The Receiving Operator Characteristic curve plots true positive rates against false positive rates. We measure the area underneath the curve the (AUC) to determine whether our model is better than a random guess, which should produce a 0.5 area. \n",
    "\n",
    "> Grid Search\n",
    "\n",
    "It goes trough an n-dimensional matrix of options in order and, in the context of sklearn, applies the various options specified in each cell to a function and returns the result.\n",
    "\n",
    "We used a a grid search with our SVMs to determine how results varied based on different kernels and regularization options.\n",
    "\n",
    "> Reflections\n",
    "\n",
    "i. I would say I have a basic grasp of the main machine learning algorithms. I think that it is stronger in the supervised learning side than the unsupervised, but that makes sense.\n",
    "\n",
    "ii. I would say that PCA was more work than immediate usefullness, mainly given the difficulty of explaining in a non-technical way to other parties (but I still think its real cool). I would say that decission trees seem really intuitive and are immediately useful.\n",
    "\n",
    "iii. I want to go deeper in to the unsupervised side. Perhaps something on neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
