{
 "metadata": {
  "name": "",
  "signature": "sha256:384600e2a0b6c149fa6b928891d8c9caaff9241e79e109ba026e2c31cbb0cfad"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "from __future__ import division\n",
      "import nltk\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "from sklearn import naive_bayes, cross_validation, linear_model\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.metrics import confusion_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Populating the interactive namespace from numpy and matplotlib\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Assignment 5 - Fresh Predictions\n",
      "*Author - Theo Love*\n",
      "*Date - 3/9/2015*\n",
      "\n",
      "This is an extension of the work done on the Rotten Tomatoes database of critic reveiws to predict the \"freshness\" of a movie.\n",
      "\n",
      "**Contents**\n",
      "1. Helper Functions\n",
      "2. Data Import\n",
      "3. Exploration of N-Grams\n",
      "4. Smarter Parsing"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##1. Helper Functions\n",
      "\n",
      "These functions are based on work done in class and will be used in the code below. In order to keep everything in one workbook, they have not been migrated to a separate module, but will probably be moved at some point in the future."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def train_and_measure(classifier, x, y, tsize=0.25, rstate=1234, print_details=False):\n",
      "    \"\"\"\n",
      "    Function accepts a classifer from sklearn and computes the accuracy measure for a random train and test split\n",
      "    and returns a tuple of (train accuracy, test accuracy)\n",
      "    classifier: an sklearn class\n",
      "    x         : a matrix of features\n",
      "    y         : a vector of targets\n",
      "    tsize     : the test size as a percent of the data (optional, default is 25%)\n",
      "    rstate    : the random number seed (optional, default is 1234)\n",
      "    \"\"\"\n",
      "    xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x, y, test_size=tsize, random_state=rstate)\n",
      "    clf = classifier.fit(xtrain, ytrain)\n",
      "\n",
      "    #Print the accuracy on the test and training dataset\n",
      "    training_accuracy = clf.score(xtrain, ytrain)\n",
      "    test_accuracy = clf.score(xtest, ytest)\n",
      "    if print_details:\n",
      "        print classifier\n",
      "        print \"Accuracy on training data: %0.4f\" % (training_accuracy)\n",
      "        print \"Accuracy on test data:     %0.4f\" % (test_accuracy)\n",
      "    \n",
      "    return (training_accuracy, test_accuracy)\n",
      "\n",
      "def test_alphas_NB(x, y, tsize=0.25, rstate=1234, alphas=[1], multi=True):\n",
      "    \"\"\"\n",
      "    Return a dataframe of train/test scores for naive-bayes models at each alpha in a list.\n",
      "    x         : a matrix of features\n",
      "    y         : a vector of targets\n",
      "    tsize     : the test size as a percent of the data (optional, default is 25%)\n",
      "    rstate    : the random number seed (optional, default is 1234)\n",
      "    alphas    : the list of alphas to test (optional, default is 1)\n",
      "    multi     : true for a Multinomial Naive-Bayes, false for Bernoulli (optional, default is True)\n",
      "    \"\"\"\n",
      "    trains = []\n",
      "    tests = []\n",
      "    for a in alphas:\n",
      "        if multi:\n",
      "            clf = naive_bayes.MultinomialNB(alpha=a)\n",
      "        else:\n",
      "            clf = naive_bayes.BernoulliNB(alpha=a)\n",
      "        r = train_and_measure(clf, x, y, tsize, rstate, print_details=False)\n",
      "        trains.append(r[0])\n",
      "        tests.append(r[1])\n",
      "\n",
      "    return pd.DataFrame({'alpha': alphas,\n",
      "                         'train_score': trains,\n",
      "                         'test_score': tests})\n",
      "\n",
      "def k_cross(classifier, x, y, nfolds=2, rstate=1234):\n",
      "    \"\"\"\n",
      "    Function that takes a classifier from sklearn and returns a tuple of the (mean, std dev) for a k-fold train/test\n",
      "    split on a dependent y vector and independent matrix x.\n",
      "    classifier: an sklearn class\n",
      "    x         : a matrix of independent variables\n",
      "    y         : a vector for the dependent variable\n",
      "    nfolds    : the number of folds in the k-fold test (optional, default is 2)\n",
      "    rstate    : the seed for the random number generator in the k-fold shuffler (optional, default is 1234)\n",
      "    \"\"\"\n",
      "    kfold = cross_validation.KFold(n=x.shape[0], n_folds=nfolds, shuffle=True, random_state=rstate)\n",
      "    train_acc = []\n",
      "    test_acc = []\n",
      "    \n",
      "    for train_index, test_index in kfold:\n",
      "        clf = classifier.fit(x[train_index], y[train_index])\n",
      "        train_acc.append(clf.score(x[train_index], y[train_index]))\n",
      "        test_acc.append(clf.score(x[test_index], y[test_index]))\n",
      "\n",
      "    return (np.array(test_acc).mean(), np.array(test_acc).std())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##2. Data Import"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# read in the data and get the length of the quotes\n",
      "critics = pd.read_csv('../DAT18NYC/data/rt_critics.csv')\n",
      "critics['quote_len'] = critics.quote.apply(lambda x: len(x))\n",
      "\n",
      "# print some pertinent data\n",
      "print \"Unique movies: %i\" % len(critics.title.unique())\n",
      "print \"Unique critics: %i\" % len(critics.critic.unique())\n",
      "print \"Fresh\\\\Rotten: %i\\\\%i\" % (len(critics[critics.fresh == 'fresh']), len(critics[critics.fresh == 'rotten']))\n",
      "print\n",
      "print \"Description of quote lengths\"\n",
      "print \"============================\"\n",
      "print critics.quote_len.describe()\n",
      "print \n",
      "print \"Columns\"\n",
      "print \"=======\"\n",
      "print critics.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Unique movies: 1736\n",
        "Unique critics: 622\n",
        "Fresh\\Rotten: 8613\\5436\n",
        "\n",
        "Description of quote lengths\n",
        "============================\n",
        "count    14072.000000\n",
        "mean       118.204946\n",
        "std         57.757129\n",
        "min          4.000000\n",
        "25%         73.000000\n",
        "50%        115.000000\n",
        "75%        160.000000\n",
        "max        256.000000\n",
        "Name: quote_len, dtype: float64\n",
        "\n",
        "Columns\n",
        "=======\n",
        "Index([u'critic', u'fresh', u'imdb', u'publication', u'quote', u'review_date', u'rtid', u'title', u'quote_len'], dtype='object')\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is also interesting to see that a little less than 40% of the reviews are rotten, meaning that there is a bias towards \"fresh\" in the reviews.\n",
      "\n",
      "The summary of the data shows that the length of each quote is a maximum of 256 characters, which is rather small, and further suggests the usage of the Bernoulli implrementation of the Naive Bayes model. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##3. Exploration of N-Grams\n",
      "\n",
      "The first new analysis to be done is to see if including additional number word groupings has any effect. Since the quotes are all rather short, the largest n-gram to be analyzed will be 5"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define the y as the fresh column with a 1 or 0\n",
      "y = (critics.fresh == 'fresh').values.astype(int)\n",
      "\n",
      "# make a sparse matrix for a number of ngrams\n",
      "x1 = CountVectorizer(ngram_range=(1,1)).fit_transform(critics.quote)\n",
      "x2 = CountVectorizer(ngram_range=(2,2)).fit_transform(critics.quote)\n",
      "x3 = CountVectorizer(ngram_range=(3,3)).fit_transform(critics.quote)\n",
      "x4 = CountVectorizer(ngram_range=(4,4)).fit_transform(critics.quote)\n",
      "x5 = CountVectorizer(ngram_range=(5,5)).fit_transform(critics.quote)\n",
      "x1to2 = CountVectorizer(ngram_range=(1,2)).fit_transform(critics.quote)\n",
      "x1to3 = CountVectorizer(ngram_range=(1,3)).fit_transform(critics.quote)\n",
      "x1to4 = CountVectorizer(ngram_range=(1,4)).fit_transform(critics.quote)\n",
      "x1to5 = CountVectorizer(ngram_range=(1,5)).fit_transform(critics.quote)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show train/test scores for the BernoulliNB classifiers with varying alpha\n",
      "print \"N-grams = 1\"\n",
      "print test_alphas_NB((x1 > 1), y, 0.4, 2345, [10**-i for i in range(0,5)], True)\n",
      "print \"N-grams = 2\"\n",
      "print test_alphas_NB((x2 > 1), y, 0.4, 2345, [10**-i for i in range(0,5)], True)\n",
      "print \"N-grams = 3\"\n",
      "print test_alphas_NB((x3 > 1), y, 0.4, 2345, [10**-i for i in range(0,5)], True)\n",
      "print \"N-grams = 4\"\n",
      "print test_alphas_NB((x4 > 1), y, 0.4, 2345, [10**-i for i in range(0,5)], True)\n",
      "print \"N-grams = 5\"\n",
      "print test_alphas_NB((x5 > 1), y, 0.4, 2345, [10**-i for i in range(0,5)], True)\n",
      "print \"N-grams = 1 to 2\"\n",
      "print test_alphas_NB((x1to2 > 1), y, 0.4, 2345, [10**-i for i in range(0,5)], True)\n",
      "print \"N-grams = 1 to 3\"\n",
      "print test_alphas_NB((x1to3 > 1), y, 0.4, 2345, [10**-i for i in range(0,5)], True)\n",
      "print \"N-grams = 1 to 4\"\n",
      "print test_alphas_NB((x1to4 > 1), y, 0.4, 2345, [10**-i for i in range(0,5)], True)\n",
      "print \"N-grams = 1 to 5\"\n",
      "print test_alphas_NB((x1to5 > 1), y, 0.4, 2345, [10**-i for i in range(0,5)], True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "N-grams = 1\n",
        "    alpha  test_score  train_score\n",
        "0  1.0000    0.612897     0.629516\n",
        "1  0.1000    0.610766     0.646690\n",
        "2  0.0100    0.610943     0.648466\n",
        "3  0.0010    0.610410     0.648111\n",
        "4  0.0001    0.609877     0.648229"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N-grams = 2\n",
        "    alpha  test_score  train_score\n",
        "0  1.0000    0.607746     0.626555\n",
        "1  0.1000    0.607390     0.627384\n",
        "2  0.0100    0.608101     0.627739\n",
        "3  0.0010    0.607568     0.627739\n",
        "4  0.0001    0.607213     0.627857"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N-grams = 3\n",
        "    alpha  test_score  train_score\n",
        "0  1.0000    0.610410     0.617198\n",
        "1  0.1000    0.610410     0.617198\n",
        "2  0.0100    0.610410     0.617198\n",
        "3  0.0010    0.610766     0.617198\n",
        "4  0.0001    0.611476     0.617198"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N-grams = 4\n",
        "    alpha  test_score  train_score\n",
        "0  1.0000    0.610055     0.614592\n",
        "1  0.1000    0.610055     0.614592\n",
        "2  0.0100    0.610055     0.614592\n",
        "3  0.0010    0.610055     0.614592\n",
        "4  0.0001    0.610233     0.614592"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N-grams = 5\n",
        "    alpha  test_score  train_score\n",
        "0  1.0000    0.610055     0.613763\n",
        "1  0.1000    0.610055     0.613763\n",
        "2  0.0100    0.610055     0.613763\n",
        "3  0.0010    0.610055     0.613763\n",
        "4  0.0001    0.610055     0.613763"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N-grams = 1 to 2\n",
        "    alpha  test_score  train_score\n",
        "0  1.0000    0.612187     0.631529\n",
        "1  0.1000    0.609167     0.648822\n",
        "2  0.0100    0.610233     0.653914\n",
        "3  0.0010    0.609877     0.653796\n",
        "4  0.0001    0.608812     0.653441"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N-grams = 1 to 3\n",
        "    alpha  test_score  train_score\n",
        "0  1.0000    0.611654     0.631292\n",
        "1  0.1000    0.608989     0.647400\n",
        "2  0.0100    0.610055     0.651783\n",
        "3  0.0010    0.609167     0.654151\n",
        "4  0.0001    0.609167     0.653441"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N-grams = 1 to 4\n",
        "    alpha  test_score  train_score\n",
        "0  1.0000    0.611832     0.631292\n",
        "1  0.1000    0.609344     0.647045\n",
        "2  0.0100    0.610055     0.650953\n",
        "3  0.0010    0.610233     0.653559\n",
        "4  0.0001    0.609344     0.653559"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "N-grams = 1 to 5\n",
        "    alpha  test_score  train_score\n",
        "0  1.0000    0.612009     0.631292\n",
        "1  0.1000    0.609700     0.646690\n",
        "2  0.0100    0.610233     0.650717\n",
        "3  0.0010    0.610943     0.653678\n",
        "4  0.0001    0.610055     0.653559"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These results show a few interesting things:\n",
      "\n",
      "1. Improvements in accuracy as alpha is decreased level out at around alpha = 0.01 (which will be used going forward)\n",
      "2. There is a slight decrease in the training score for single level n-grams, but not much of an affect on the test score. In general the testing score stays at 61%, and the range between the two gets tighter.\n",
      "3. There is not much improvement in accuracy for including higher ngrams, and in general the accuracy levels out at about 65% train / 61% test, which is still a nice tight range.\n",
      "\n",
      "Next we will examine confusion matrices"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# show confusion matrix for each of the ngram x sets\n",
      "print \"Bernoulli Confusion Matrix\"\n",
      "print \"[[False, FalsePos]\"\n",
      "print \" [FalseNeg, True]]\"\n",
      "print\n",
      "clf = naive_bayes.BernoulliNB(alpha=0.01)\n",
      "\n",
      "print \"Constant Size NGrams\"\n",
      "print \"====================\"\n",
      "print \"NGrams = 1\"\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x1, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x1))\n",
      "print \"NGrams = 2\"\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x2, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x2))\n",
      "print \"NGrams = 3\"\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x3, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x3))\n",
      "print \"NGrams = 4\"\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x4, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x4))\n",
      "print \"NGrams = 5\"\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x5, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x5))\n",
      "print\n",
      "\n",
      "print \"Increasing Size NGrams\"\n",
      "print \"======================\"\n",
      "print \"NGrams = 1 to 2\"\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x1to2, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x1to2))\n",
      "print \"NGrams = 1 to 3\"\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x1to3, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x1to3))\n",
      "print \"NGrams = 1 to 4\"\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x1to4, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x1to4))\n",
      "print \"NGrams = 1 to 5\"\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x1to5, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x1to5))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Bernoulli Confusion Matrix\n",
        "[[False, FalsePos]\n",
        " [FalseNeg, True]]\n",
        "\n",
        "Constant Size NGrams\n",
        "====================\n",
        "NGrams = 1\n",
        "[[3507 1952]\n",
        " [4458 4155]]\n",
        "NGrams = 2\n",
        "[[4773  686]\n",
        " [7159 1454]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NGrams = 3\n",
        "[[5250  209]\n",
        " [8220  393]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NGrams = 4\n",
        "[[5150  309]\n",
        " [8134  479]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NGrams = 5\n",
        "[[5012  447]\n",
        " [7932  681]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "Increasing Size NGrams\n",
        "======================\n",
        "NGrams = 1 to 2\n",
        "[[4503  956]\n",
        " [6284 2329]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NGrams = 1 to 3\n",
        "[[5082  377]\n",
        " [7518 1095]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NGrams = 1 to 4\n",
        "[[5252  207]\n",
        " [8019  594]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "NGrams = 1 to 5\n",
        "[[5319  140]\n",
        " [8203  410]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "While the accuracy of the training went up slightly with more ngrams, the test accuracy remained similar. Drilling down with a confusion matrix we see a much clearer trend of a major issue with false negatives, and a drastic cut in the number of true guesses.\n",
      "\n",
      "Essentially, the main difference between the models is that they are all aorund 61% accurate, but including additional n-grams tilts results drastically negative and makes the number of false negatives through the roof.\n",
      "\n",
      "It is also worth pointing out, however that this is essentially as good a performance as always choosing \"fresh\"...\n",
      "\n",
      "Let's see if we can do some additional wrangling to help with our false-negative problem."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##4. Smarter Parsing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test to see if the stops words matter\n",
      "clf = naive_bayes.BernoulliNB(alpha=0.01)\n",
      "x_new = CountVectorizer(ngram_range=(1,2), stop_words='english').fit_transform(critics.quote)\n",
      "\n",
      "print \"Base 1 to 2 NGrams\"\n",
      "print \"==================\"\n",
      "train_and_measure(clf, (x1to2 > 1), y, tsize=0.4, print_details=True)\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x1to2, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x1to2))\n",
      "print\n",
      "print \"W/o Stop Words\"\n",
      "print \"==================\"\n",
      "train_and_measure(clf, (x_new > 1), y, tsize=0.4, print_details=True)\n",
      "xtrain, xtest, ytrain, ytest = cross_validation.train_test_split(x_new, y, test_size=.4, random_state=1234)\n",
      "clf.fit((xtrain > 1), ytrain)\n",
      "print confusion_matrix(y, clf.predict(x_new))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Base 1 to 2 NGrams\n",
        "==================\n",
        "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "Accuracy on training data: 0.6525\n",
        "Accuracy on test data:     0.6090\n",
        "[[4503  956]\n",
        " [6284 2329]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "W/o Stop Words\n",
        "==================\n",
        "BernoulliNB(alpha=0.01, binarize=0.0, class_prior=None, fit_prior=True)\n",
        "Accuracy on training data: 0.6405\n",
        "Accuracy on test data:     0.6060\n",
        "[[4694  765]\n",
        " [6801 1812]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stop words, at least from the sklearn package, actually make the fit worse, so don't help much. The next section gets the sentiment score for each quote, but since it takes a long time to run, I recommend using the next code chunk to just import the finished data.\n",
      "\n",
      "**Warning (the next code section took 20 minutes so skip it to import just the finished dataset**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# let us use the text2sentiment tool to get a sentiment score for each quote\n",
      "import json\n",
      "import requests\n",
      "\n",
      "def sentiment_score(sentence):\n",
      "    url = 'http://www.datasciencetoolkit.org/text2sentiment/'\n",
      "    payload = {'text': sentence} # The sentence we want the sentiment of \n",
      "    headers = {'content-type': 'application/json'} # The type of data you are sending\n",
      "    r = requests.post(url, data=json.dumps(payload), headers=headers) # Send the data\n",
      "    return sentence, json.loads(r.text)['score'] # Print the results\n",
      "\n",
      "critics['quote_score'] = critics.quote.apply(lambda x: sentiment_score(x))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "######\n",
      "# IMPORT COMPLETED DATA\n",
      "######\n",
      "c = pd.read_csv('critics_tl.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "re.sub()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "sub() takes at least 3 arguments (0 given)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-c2beaab5ae1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m: sub() takes at least 3 arguments (0 given)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import linear_model\n",
      "\n",
      "clf = linear_model.LinearRegression()\n",
      "lm1 = clf.fit(np.array(crit_new[['qs']]), np.array(crit_new.f))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print lm1.score(np.array(crit_new[['qs']]), np.array(crit_new.f))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0601167763986\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xn = np.array(crit_new[['qs']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'crit_new' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-5-a4dd948a710a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrit_new\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'qs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'crit_new' is not defined"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import statsmodels.formula.api as smf\n",
      "\n",
      "model = smf.ols(formula='f ~ qs', data=crit_new)\n",
      "results = model.fit()\n",
      "print 'NORMAL FIT SUMMARY'\n",
      "print(results.summary())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "NORMAL FIT SUMMARY\n",
        "                            OLS Regression Results                            \n",
        "==============================================================================\n",
        "Dep. Variable:                      f   R-squared:                       0.060\n",
        "Model:                            OLS   Adj. R-squared:                  0.060\n",
        "Method:                 Least Squares   F-statistic:                     899.9\n",
        "Date:                Mon, 09 Mar 2015   Prob (F-statistic):          1.03e-191\n",
        "Time:                        18:00:06   Log-Likelihood:                -9414.5\n",
        "No. Observations:               14072   AIC:                         1.883e+04\n",
        "Df Residuals:                   14070   BIC:                         1.885e+04\n",
        "Df Model:                           1                                         \n",
        "==============================================================================\n",
        "                 coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
        "------------------------------------------------------------------------------\n",
        "Intercept      0.5645      0.004    131.676      0.000         0.556     0.573\n",
        "qs             0.0743      0.002     29.999      0.000         0.069     0.079\n",
        "==============================================================================\n",
        "Omnibus:                      378.939   Durbin-Watson:                   1.503\n",
        "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             1861.853\n",
        "Skew:                          -0.418   Prob(JB):                         0.00\n",
        "Kurtosis:                       1.426   Cond. No.                         1.98\n",
        "==============================================================================\n"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}